{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d43a9a5-f998-4460-a584-a7bbbdaef3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39aaea19-aa0d-47a0-a346-89aadbee0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "from agent import MyAgent\n",
    "from models import DRQNetwork, MixingNetwork\n",
    "from transferlearning import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56a3715f-fbc0-4b93-87b2-58b3426a1f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using QMIX agent with mixer network\n",
      "DRQNetwork(\n",
      "  (feature_layer): Sequential(\n",
      "    (0): Linear(in_features=42, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (gru): GRU(512, 512, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=512, out_features=7, bias=True)\n",
      ")\n",
      "MixingNetwork(\n",
      "  (hyper_w1): Sequential(\n",
      "    (0): Linear(in_features=949, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  )\n",
      "  (hyper_w2): Sequential(\n",
      "    (0): Linear(in_features=949, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (hyper_b1): Linear(in_features=949, out_features=512, bias=True)\n",
      "  (hyper_b2): Sequential(\n",
      "    (0): Linear(in_features=949, out_features=512, bias=True)\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Episode 1, Step 57, Reward: -54005.80, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 2, Step 30, Reward: -66002.32, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 3, Step 45, Reward: -97002.57, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 4, Step 88, Reward: -202006.17, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 5, Step 86, Reward: -194006.85, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 6, Step 51, Reward: -94001.53, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 7, Step 28, Reward: -32003.63, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 8, Step 268, Reward: -460020.95, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 9, Step 22, Reward: -37002.27, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 3 due to space constraints\n",
      "Episode 10, Step 132, Reward: -361001.61, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 11, Step 19, Reward: -24001.34, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 12, Step 52, Reward: -51003.48, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 13, Step 18, Reward: -39001.03, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 14, Step 65, Reward: -67003.04, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 15, Step 121, Reward: -297003.87, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 16, Step 29, Reward: -33001.37, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 17, Step 30, Reward: -53001.45, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 18, Step 34, Reward: -56002.48, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 4 due to space constraints\n",
      "Episode 19, Step 55, Reward: -107001.82, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 20, Step 18, Reward: -30000.31, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 21, Step 43, Reward: -87001.09, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 22, Step 31, Reward: -76001.49, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 23, Step 16, Reward: -25001.72, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 4 due to space constraints\n",
      "Episode 24, Step 28, Reward: -54000.60, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 25, Step 81, Reward: -191002.65, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 26, Step 110, Reward: -98985.99, Evacuated: 2, Deactivated: 2, MEAN TD LOSS: 0.00e+00\n",
      "Episode 27, Step 36, Reward: -74002.00, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 28, Step 5, Reward: -9000.16, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 29, Step 32, Reward: -45003.95, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 30, Step 56, Reward: -72002.44, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 31, Step 74, Reward: -134006.01, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 32, Step 47, Reward: -57005.80, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 33, Step 45, Reward: -14970.43, Evacuated: 3, Deactivated: 1, MEAN TD LOSS: 0.00e+00\n",
      "Episode 34, Step 52, Reward: -87991.50, Evacuated: 1, Deactivated: 3, MEAN TD LOSS: 0.00e+00\n",
      "Episode 35, Step 50, Reward: -79004.81, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 36, Step 58, Reward: -134990.41, Evacuated: 1, Deactivated: 3, MEAN TD LOSS: 0.00e+00\n",
      "Episode 37, Step 6, Reward: -9000.42, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 1 due to space constraints\n",
      "Episode 38, Step 27, Reward: -34000.84, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 39, Step 60, Reward: -136004.31, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 4 due to space constraints\n",
      "Episode 40, Step 25, Reward: -50000.50, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 41, Step 22, Reward: -44000.86, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 42, Step 55, Reward: -135003.21, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 1 due to space constraints\n",
      "Episode 43, Step 38, Reward: -101990.35, Evacuated: 1, Deactivated: 3, MEAN TD LOSS: 0.00e+00\n",
      "Episode 44, Step 48, Reward: -26005.83, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 45, Step 18, Reward: -46000.50, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 46, Step 29, Reward: -49001.98, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 1 due to space constraints\n",
      "Episode 47, Step 12, Reward: -26000.32, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 48, Step 37, Reward: -72002.27, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 49, Step 68, Reward: -190001.16, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 50, Step 31, Reward: -20001.90, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 51, Step 33, Reward: -58001.29, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 52, Step 16, Reward: -26001.13, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 53, Step 64, Reward: -171002.19, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 54, Step 43, Reward: -61990.95, Evacuated: 1, Deactivated: 3, MEAN TD LOSS: 0.00e+00\n",
      "Episode 55, Step 38, Reward: -60004.38, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 56, Step 33, Reward: -49002.39, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 4 due to space constraints\n",
      "Episode 57, Step 31, Reward: -73000.39, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 1 due to space constraints\n",
      "Episode 58, Step 37, Reward: -41000.78, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 59, Step 73, Reward: -164003.99, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 60, Step 68, Reward: -166002.89, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 61, Step 51, Reward: -100991.39, Evacuated: 1, Deactivated: 3, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 1 due to space constraints\n",
      "Episode 62, Step 50, Reward: -82000.91, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Episode 63, Step 30, Reward: -42001.92, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 0.00e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 64, Step 77, Reward: -26970.83, Evacuated: 3, Deactivated: 1, MEAN TD LOSS: 0.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1139: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /home/conda/feedstock_root/build_artifacts/libtorch_1728241823685/work/aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65, Step 14, Reward: -34000.88, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 2.89e+01\n",
      "Episode 66, Step 122, Reward: -112977.44, Evacuated: 3, Deactivated: 1, MEAN TD LOSS: 1.99e+01\n",
      "Episode 67, Step 58, Reward: -84003.42, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 1.70e+01\n",
      "Episode 68, Step 24, Reward: -56001.26, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 1.59e+01\n",
      "Episode 69, Step 500, Reward: -719056.45, Evacuated: 0, Deactivated: 2, MEAN TD LOSS: 1.31e+01\n",
      "Episode 70, Step 15, Reward: -20001.41, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 1.22e+01\n",
      "Warning: Could only place 0 dynamic obstacles instead of 4 due to space constraints\n",
      "Episode 71, Step 12, Reward: -29000.21, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 1.05e+01\n",
      "Episode 72, Step 18, Reward: -23001.94, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 1.07e+01\n",
      "Episode 73, Step 500, Reward: -1051046.08, Evacuated: 0, Deactivated: 3, MEAN TD LOSS: 1.04e+01\n",
      "Episode 74, Step 500, Reward: -478068.32, Evacuated: 0, Deactivated: 1, MEAN TD LOSS: 9.24e+00\n",
      "Episode 75, Step 11, Reward: -26000.40, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 8.94e+00\n",
      "Warning: Could only place 0 dynamic obstacles instead of 2 due to space constraints\n",
      "Episode 76, Step 4, Reward: -8000.10, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 9.37e+00\n",
      "Episode 77, Step 500, Reward: -882043.58, Evacuated: 0, Deactivated: 3, MEAN TD LOSS: 8.13e+00\n",
      "Episode 78, Step 500, Reward: -943016.17, Evacuated: 0, Deactivated: 2, MEAN TD LOSS: 7.73e+00\n",
      "Episode 79, Step 4, Reward: -8000.18, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 8.30e+00\n",
      "Episode 80, Step 500, Reward: -247086.71, Evacuated: 0, Deactivated: 1, MEAN TD LOSS: 7.33e+00\n",
      "Episode 81, Step 500, Reward: -276080.71, Evacuated: 0, Deactivated: 1, MEAN TD LOSS: 7.08e+00\n",
      "Episode 82, Step 4, Reward: -8000.22, Evacuated: 0, Deactivated: 4, MEAN TD LOSS: 5.15e+00\n",
      "Episode 83, Step 459, Reward: -214082.10, Evacuated: 0, Deactivated: 2, MEAN TD LOSS: 7.07e+00\n",
      "Simulation interrupted by the user\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cloned_policy_path = \"./cloned_policy\"\n",
    "pretrained_mixer_path = \"./pretrained_mixer\"\n",
    "\n",
    "fine_tuned_policy_path = \"./fine_tuned_policy\"\n",
    "fine_tuned_mixer_path = \"./fine_tuned_mixer\"\n",
    "\n",
    "cloned_policy_net = DRQNetwork.load(cloned_policy_path).to(device)\n",
    "cloned_policy_net.train()\n",
    "cloned_policy_net.gru.flatten_parameters()\n",
    "\n",
    "pretrained_mixer = MixingNetwork.load(pretrained_mixer_path).to(device)\n",
    "pretrained_mixer.train()\n",
    "\n",
    "my_agent = MyAgent(\n",
    "    num_agents=4,\n",
    "    device=device,\n",
    "    policy_net=cloned_policy_net,\n",
    "    mixing_net=pretrained_mixer,\n",
    "    buffer_size=2000,\n",
    "    batch_sequence_length=20,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    gamma=0.99,\n",
    "    epsilon=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.9995,\n",
    "    target_update_freq=1,\n",
    "    tau=1e-3,\n",
    "    gradient_clipping_value=1,\n",
    ")\n",
    "\n",
    "my_agent.optimizer = optim.AdamW(\n",
    "    list(my_agent.policy_net.parameters()) + list(my_agent.mixer.parameters()),\n",
    "    lr=1e-5,\n",
    "    weight_decay=1e-5,\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "trained_agent, all_rewards, all_losses = train(my_agent, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546c1e8d-5c0d-4664-a66a-d10b31cfdd18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
